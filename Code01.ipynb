{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb60fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5709d6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_structure_data(json_file_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Check if data is a dictionary containing a list\n",
    "    if isinstance(data, dict):\n",
    "        # Look for the list within the dictionary keys\n",
    "        # Replace 'conversations' with the actual top-level key in your JSON\n",
    "        conversations_list = data.get('conversations', []) \n",
    "    else:\n",
    "        conversations_list = data\n",
    "\n",
    "    flattened_records = []\n",
    "    \n",
    "    for conversation in conversations_list:\n",
    "        # Ensure we are dealing with a dictionary [cite: 19]\n",
    "        if not isinstance(conversation, dict):\n",
    "            continue\n",
    "            \n",
    "        call_id = conversation.get('call_id')\n",
    "        outcome = conversation.get('outcome_event')\n",
    "        \n",
    "        # Flattening turns to maintain speaker labels and sequence \n",
    "        for turn in conversation.get('transcript', []):\n",
    "            flattened_records.append({\n",
    "                \"call_id\": call_id,\n",
    "                \"outcome_event\": outcome,\n",
    "                \"speaker\": turn.get('speaker'),\n",
    "                \"text\": turn.get('text'),\n",
    "                \"turn_id\": turn.get('turn_id')\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(flattened_records)\n",
    "\n",
    "# Execution\n",
    "df = load_and_structure_data('Conversational_Transcript_Dataset.json')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19e8b39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'dict'>\n",
      "DataFrame is still empty. Please check the JSON keys.\n"
     ]
    }
   ],
   "source": [
    "def load_and_structure_data(json_file_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Debug: Check what the top-level structure is\n",
    "    print(f\"Data type: {type(data)}\")\n",
    "    \n",
    "    # If it's a dict, find the list of conversations\n",
    "    if isinstance(data, dict):\n",
    "        # Common keys are 'conversations', 'data', or 'calls'\n",
    "        for key in ['conversations', 'data', 'calls']:\n",
    "            if key in data:\n",
    "                conversations_list = data[key]\n",
    "                break\n",
    "        else:\n",
    "            # If no key found, check if the dict itself contains one conversation\n",
    "            conversations_list = [data] if 'call_id' in data else []\n",
    "    else:\n",
    "        conversations_list = data\n",
    "\n",
    "    flattened_records = []\n",
    "    \n",
    "    for conversation in conversations_list:\n",
    "        call_id = conversation.get('call_id')\n",
    "        outcome = conversation.get('outcome_event')\n",
    "        \n",
    "        # Access the transcript list [cite: 8, 19]\n",
    "        transcript = conversation.get('transcript', [])\n",
    "        \n",
    "        for turn in transcript:\n",
    "            flattened_records.append({\n",
    "                \"call_id\": call_id,\n",
    "                \"outcome_event\": outcome,\n",
    "                \"speaker\": turn.get('speaker'),\n",
    "                \"text\": turn.get('text'),\n",
    "                \"turn_id\": turn.get('turn_id')\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(flattened_records)\n",
    "\n",
    "# Run this and check the output\n",
    "df = load_and_structure_data('Conversational_Transcript_Dataset.json')\n",
    "if df.empty:\n",
    "    print(\"DataFrame is still empty. Please check the JSON keys.\")\n",
    "else:\n",
    "    print(f\"Successfully loaded {len(df)} turns.\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6acac9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Structure Discovery ---\n",
      "Conversation Keys: ['transcript_id', 'time_of_interaction', 'domain', 'intent', 'reason_for_call', 'conversation']\n",
      "Likely transcript key: conversation\n",
      "\n",
      "--- Success! ---\n",
      "  call_id outcome_event   speaker  \\\n",
      "0    None          None     Agent   \n",
      "1    None          None  Customer   \n",
      "2    None          None     Agent   \n",
      "3    None          None  Customer   \n",
      "4    None          None     Agent   \n",
      "\n",
      "                                                text turn_id  \n",
      "0  Hello, thank you for contacting BuyNow. This i...    None  \n",
      "1  Hello, I'm calling about an order that shows d...    None  \n",
      "2  I'm sorry to hear that. I'll definitely help y...    None  \n",
      "3  It's 9595912. The tracking was marked delivere...    None  \n",
      "4  Let me pull that up right away. Okay, I see th...    None  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def fix_and_load_data(json_file_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Access the main list\n",
    "    convs = data.get('transcripts', [])\n",
    "    \n",
    "    if not convs:\n",
    "        print(f\"Key 'transcripts' not found. Available keys are: {list(data.keys())}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # DEBUG: Let's see what one conversation looks like\n",
    "    print(\"--- Structure Discovery ---\")\n",
    "    sample = convs[0]\n",
    "    print(f\"Conversation Keys: {list(sample.keys())}\")\n",
    "    \n",
    "    # Find the transcript key inside the conversation\n",
    "    # It might be 'transcript', 'dialogue', 'turns', etc.\n",
    "    t_key = next((k for k in sample.keys() if isinstance(sample[k], list)), None)\n",
    "    print(f\"Likely transcript key: {t_key}\")\n",
    "    \n",
    "    flattened_records = []\n",
    "    for conversation in convs:\n",
    "        # We use .get() with fallback to handle potential missing data \n",
    "        cid = conversation.get('call_id') or conversation.get('id')\n",
    "        out = conversation.get('outcome_event') or conversation.get('outcome')\n",
    "        \n",
    "        # Use the discovered transcript key\n",
    "        turns = conversation.get(t_key, [])\n",
    "        \n",
    "        for turn in turns:\n",
    "            flattened_records.append({\n",
    "                \"call_id\": cid,\n",
    "                \"outcome_event\": out,\n",
    "                \"speaker\": turn.get('speaker') or turn.get('role'),\n",
    "                \"text\": turn.get('text') or turn.get('content'),\n",
    "                \"turn_id\": turn.get('turn_id') or turn.get('index')\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(flattened_records)\n",
    "\n",
    "df = fix_and_load_data('Conversational_Transcript_Dataset.json')\n",
    "if not df.empty:\n",
    "    print(\"\\n--- Success! ---\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce03d700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 84465 turns across 5037 unique calls.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_structure_data(json_file_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    convs = data.get('transcripts', [])\n",
    "    flattened_records = []\n",
    "    \n",
    "    for conversation in convs:\n",
    "        # Mapping your specific keys\n",
    "        call_id = conversation.get('transcript_id') # From your 'transcript_id'\n",
    "        outcome = conversation.get('intent') # Or 'reason_for_call' [cite: 5]\n",
    "        \n",
    "        turns = conversation.get('conversation', []) # From your 'conversation' key\n",
    "        \n",
    "        for turn in turns:\n",
    "            flattened_records.append({\n",
    "                \"call_id\": call_id,\n",
    "                \"outcome_event\": outcome,\n",
    "                \"speaker\": turn.get('speaker'),\n",
    "                \"text\": turn.get('text'),\n",
    "                \"turn_id\": turn.get('turn_id')\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(flattened_records)\n",
    "\n",
    "df = load_and_structure_data('Conversational_Transcript_Dataset.json')\n",
    "print(f\"Loaded {len(df)} turns across {df['call_id'].nunique()} unique calls.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "921763bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index successfully built with 84465 dialogue turns.\n",
      "                   call_id speaker  \\\n",
      "52896  4265-9695-7361-8662   Agent   \n",
      "9775   6043-7841-9619-4424   Agent   \n",
      "1819   7038-2056-8606-6726   Agent   \n",
      "\n",
      "                                                    text  \\\n",
      "52896  I'm sorry to hear about the delay. Let me chec...   \n",
      "9775   I'm sorry to hear about the delay. Let me chec...   \n",
      "1819   I'm sorry to hear about the delay. Let me chec...   \n",
      "\n",
      "                                         outcome_event  \n",
      "52896  Multiple Issues - Order Status & Account Access  \n",
      "9775   Multiple Issues - Order Status & Account Access  \n",
      "1819   Multiple Issues - Order Status & Account Access  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class EvidenceIndexer:\n",
    "    def __init__(self):\n",
    "        # TF-IDF is built into sklearn (no extra installation usually needed)\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        self.tfidf_matrix = None\n",
    "        self.metadata = None\n",
    "\n",
    "    def create_index(self, dataframe):\n",
    "        # We must work over a corpus of conversational transcripts \n",
    "        self.metadata = dataframe.reset_index(drop=True)\n",
    "        # Handle noisy conversational data by filling NAs [cite: 9]\n",
    "        text_data = self.metadata['text'].fillna(\"\")\n",
    "        \n",
    "        # This builds the indexing mechanism \n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(text_data)\n",
    "        print(f\"Index successfully built with {self.tfidf_matrix.shape[0]} dialogue turns.\")\n",
    "\n",
    "    def get_evidence(self, query, top_k=3):\n",
    "        # Convert user query to the same TF-IDF space\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        \n",
    "        # Calculate similarity between query and all dialogue turns\n",
    "        cosine_sim = cosine_similarity(query_vec, self.tfidf_matrix).flatten()\n",
    "        \n",
    "        # Get the top_k most relevant indices\n",
    "        relevant_indices = cosine_sim.argsort()[-top_k:][::-1]\n",
    "        \n",
    "        # Extract specific dialogue spans that serve as supporting evidence \n",
    "        results = self.metadata.iloc[relevant_indices].copy()\n",
    "        \n",
    "        # Ensure the output is traceable back to concrete evidence [cite: 9]\n",
    "        return results[['call_id', 'speaker', 'text', 'outcome_event']]\n",
    "\n",
    "# --- EXECUTION ---\n",
    "indexer = EvidenceIndexer()\n",
    "indexer.create_index(df)\n",
    "\n",
    "# Test with a query to ensure it returns identifiable portions of data [cite: 22]\n",
    "print(indexer.get_evidence(\"customer delivery delay complaint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbfaf9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
